{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load in\n\n#Install these two packages\n#------------------------------------------------\n# !pip install scispacy\n# !pip install allennlp\n#------------------------------------------------\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom matplotlib import pyplot as plt\nimport seaborn as sns\n# import scispacy\n# import spacy\nplt.style.use('fivethirtyeight')\n# Input data files are available in the \"../input/\" directory.\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\n# import os\n# for dirname, _, filenames in os.walk('/kaggle/input'):\n#     for filename in filenames:\n#         print(os.path.join(dirname, filename))\n\n# Any results you write to the current directory are saved as output.","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"d629ff2d2480ee46fbb7e2d37f6b5fab8052498a","_cell_guid":"79c7e3d0-c299-4dcb-8224-4455121ee9b0","trusted":true},"cell_type":"code","source":"data = pd.read_csv('/kaggle/input/CORD-19-research-challenge/2020-03-13/all_sources_metadata_2020-03-13.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.head(3)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.columns","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data.isnull().any()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"data['source_x'].value_counts().plot.bar()\nplt.title(\"Source of Paper\")\nplt.ylabel(\"Count\")\nplt.xlabel(\"Source Name\")\nplt.show()\n#PMC:PubMed Central is a free digital repository that archives publicly accessible full-text \n#scholarly articles that have been published within the biomedical and life sciences journal literature.","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Last 20 years frequencies\nplt.title(\"Frequency of published papers (Last 20yr.)\")\npublish_time = data['publish_time'].dropna()\npublish_time = publish_time.apply(lambda x: int(x[:4])) #get the year\npublish_time_last20yr = publish_time[publish_time >= 2000]\nsns.set(style=\"whitegrid\")\nax = sns.countplot(publish_time_last20yr)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#20th century\nplt.title(\"Frequency of published papers (20th century)\")\npublish_time_20th = publish_time[publish_time < 2000]\nsns.set(style=\"whitegrid\")\nax = sns.countplot(publish_time_20th)\nplt.xlabel(\"Year\")\nplt.ylabel(\"Frequency\")\nplt.xticks(rotation=90)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Download SciSpacy en_core_sci_lg Model\n#A full spaCy pipeline for biomedical data with a ~785k vocabulary and 600k word vectors.\n!wget --header=\"Host: s3-us-west-2.amazonaws.com\" --header=\"User-Agent: Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_3) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/80.0.3987.132 Safari/537.36\" --header=\"Accept: text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9\" --header=\"Accept-Language: en-US,en;q=0.9\" --header=\"Referer: https://github.com/allenai/scispacy\" \"https://s3-us-west-2.amazonaws.com/ai2-s2-scispacy/releases/v0.2.4/en_core_sci_lg-0.2.4.tar.gz\" -O \"en_core_sci_lg-0.2.4.tar.gz\" -c\n\n#Extract the .tar.gz file\nimport tarfile\nfname=\"/kaggle/working/en_core_sci_lg-0.2.4.tar.gz\"\nif fname.endswith(\"tar.gz\"):\n    tar = tarfile.open(fname, \"r:gz\")\n    tar.extractall()\n    tar.close()\nelif fname.endswith(\"tar\"):\n    tar = tarfile.open(fname, \"r:\")\n    tar.extractall()\n    tar.close()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"#Broken due to allennlp\n\n# #NER\n# from spacy import displacy\n# nlp = scispacy.load(\"en_core_web_lg\") #Spacy\n# # print(\"Text : \", data[\"title\"][10])\n# # doc = nlp(data[\"title\"][10])\n# # displacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# print(\"Text : \", data[\"abstract\"][2])\n# doc = nlp(data[\"abstract\"][2])\n# displacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"nlp = spacy.load(\"/kaggle/working/en_core_sci_lg-0.2.4/en_core_sci_lg/en_core_sci_lg-0.2.4/\") #SciSpacy\ndoc = nlp(data[\"abstract\"][2])\ndisplacy.render(doc, style='ent')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"titles = data['title'].dropna().values\nabstracts = data['abstract'].dropna().values","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"from spacy.tokenizer import Tokenizer\ntokenizer = Tokenizer(vocab=nlp.vocab)\ntokenized = tokenizer(' '.join(abstracts))\nprint(\"Number of tokenized words: \", len(tokenized))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from collections import Counter\nfrom tqdm import tqdm_notebook\nvocab = Counter()\nfor token in tqdm_notebook(tokenized):\n    vocab[token.text] += 1","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"print(\"Number of unique tokens:\", len(vocab))\nfrom nltk.corpus import stopwords\nstopword = stopwords.words('english')\nvocab_pure = Counter()\nfor k, v in vocab.items():\n    if k.lower() not in stopword:\n        vocab_pure[k] = v\nprint(\"Number of unique tokens (after stopword removal)\", len(vocab_pure))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 20\nx = [name[0].lower() for name in vocab_pure.most_common(k)]\ny = [value[1] for value in vocab_pure.most_common(k)]\nplt.figure(figsize=(7, 7))\nplt.bar(x, y)\nplt.title(f\"{k} Most common words (abstract)\")\nplt.xticks(rotation=60, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"tokenized = tokenizer(' '.join(titles))\nprint(\"Number of tokenized words: \", len(tokenized))\nvocab = Counter()\nfor token in tqdm_notebook(tokenized):\n    vocab[token.text] += 1\nprint(\"Number of unique tokens:\", len(vocab))\nvocab_pure = Counter()\nfor k, v in vocab.items():\n    if k.lower() not in stopword:\n        vocab_pure[k] = v\nprint(\"Number of unique tokens (after stopword removal)\", len(vocab_pure))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"k = 20\nx = [name[0].lower() for name in vocab_pure.most_common(k)]\ny = [value[1] for value in vocab_pure.most_common(k)]\nplt.figure(figsize=(7, 7))\nplt.bar(x, y)\nplt.title(f\"{k} Most common words (titles)\")\nplt.xticks(rotation=60, fontsize=15)\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from wordcloud import WordCloud, STOPWORDS\nstopwords = set(STOPWORDS)\nplt.figure(figsize=(15, 15))\nwordcloud = WordCloud(\n            background_color='black',\n            stopwords=stopwords,\n            max_words=200,\n            random_state=42, scale=100\n            ).generate(' '.join(titles))\nplt.imshow(wordcloud)\nplt.axis('off')\nplt.show()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"collapsed":true},"cell_type":"code","source":"!pip install allennlp\nfrom allennlp.predictors.predictor import Predictor\npredictor = Predictor.from_path(\"https://s3-us-west-2.amazonaws.com/allennlp/models/bidaf-model-2017.09.15-charpad.tar.gz\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"passage = ''\nfor abstract in data['abstract'].dropna().values:\n    passage += ' '+ abstract","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"limit = int(1e4)\nresult = predictor.predict(passage=passage[:limit], question='Where did the virus originate from?')\nresult['best_span_str']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"#Source: https://www.kaggle.com/shahules/eda-find-similar-papers-easily/log\nimport tensorflow_hub as hub\nimport tensorflow as tf\nimport gensim\n\ndef prepare_similarity(vectors):\n    similarity=cosine_similarity(vectors)\n    return similarity\n\ndef get_top_similar(sentence, sentence_list, similarity_matrix, topN):\n    # find the index of sentence in list\n    index = sentence_list.index(sentence)\n    # get the corresponding row in similarity matrix\n    similarity_row = np.array(similarity_matrix[index, :])\n    # get the indices of top similar\n    indices = similarity_row.argsort()[-topN:][::-1]\n    return [sentence_list[i] for i in indices]\n\n\n\ndata = data[['abstract', 'title']].dropna()\nmodule_url = \"../input/universalsentenceencoderlarge4\" \n# Import the Universal Sentence Encoder's TF Hub module\nembed = hub.load(module_url)\n\nlimit = 7000\ntitles = data['title'].dropna()\nembed_vectors = embed(titles[:limit].values)['outputs'].numpy()\nsentence_list = titles.values.tolist()\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.metrics.pairwise import cosine_similarity\nsentence = titles.iloc[1]\nprint(\"Find similar research papers for :\")\nprint(sentence)\n\nsimilarity_matrix=prepare_similarity(embed_vectors)\nsimilar=get_top_similar(sentence,sentence_list,similarity_matrix,6)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"for sentence in similar:\n    print(sentence)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"result['best_span_str']","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"predictor.predict(passage=passage[:limit], question='What is the epicenter?')['best_span_str']","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}